{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13f6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class dense_layer:\n",
    "    def __init__ (self, n_inputs, n_neurons):\n",
    "        #weights and biases are initalized as transposed already\n",
    "        self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a910f7",
   "metadata": {},
   "source": [
    "**Activation functions** are applied to outputs, so that it is possible for a 2+ layers model to map nonlinear functions.  \n",
    "An NN generally has two types of activation functions: the one used in hidden layers, and the second the one used in the output layer.  \n",
    "### Why?\n",
    "Activation function, as we said, are used so that a NN can map nonlinearity. The main idea is that a nonlinear function cannot be represented by a straight line (one such function is the sine function). Problems in real life usually are represented by non linearity, as they are pretty complex.  \n",
    "If we do not use activation functions, the final results would only represent y=x, as neurons by themselves represent just the sum of products + bias.\n",
    "\n",
    "\n",
    "We have several choices:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3464f",
   "metadata": {},
   "source": [
    "### The step activation function\n",
    "\n",
    "The purpose of this activation function is to mimic a neuron \"firing\" or \"not firing\" based on some input information. The simplest version is a step function, where if the output is greater than 0, the neuron will output 1; otherwise it will output 0. This is rarely used nowadays.\n",
    "\n",
    "### The linear activation function\n",
    "\n",
    "A linear activation function is the equation of a line, for instance x=y. This is usually applied to the last layer's output for regression models\n",
    "\n",
    "### The sigmoid activation function\n",
    "\n",
    "The problem with the step function is that it is not informative, i.e. we lose information about the output, as it always gives either 0 or 1 as a result. It is better to have a function with more granular and informative results. The original solution for this problem is the **sigmoid** activation function. By using this, we can \"trace back\" to the original output value. \n",
    "\n",
    "### Rectified linear activation function\n",
    "the **rectified linear unit activation function** (ReLU) is a simpler implementation, where we have y=x for values > 0, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4653e4",
   "metadata": {},
   "source": [
    "---\n",
    "## Coding ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa98506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n",
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0,2,-1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "print(output)\n",
    "\n",
    "output = np.maximum(0, inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3356067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "class activation_relu:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "dense1 = dense_layer(2,3)\n",
    "\n",
    "activation1 = activation_relu()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "print(activation1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7452f39a",
   "metadata": {},
   "source": [
    "### The softmax activation function\n",
    "\n",
    "Since we want our model to be a classifier, we want our activation function to work for classification. One of such function is the **softmax** function. In this case, the issue with ReLU is that it is:\n",
    "1. not normalized (i.e., the values can be anything)\n",
    "2. exclusive (i.e., each output is inde√®endent of the others)\n",
    "In case of classification, we want the ouput to reflect a prediction of which class the network thinks the input represents. A softmax activation function represents **confidence scores** for each class, which will add up to 1. For instance, we could have an output of \\[0.45, 0.55\\], which would predict the 2nd class. We can also calculate the confidence distribution, which in this case would not be very high.  \n",
    "Let's code this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b106f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041752   3.35348465  10.85906266]\n",
      "[0.89528266 0.02470831 0.08000903]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "# first we exponentiate the outputs\n",
    "exp_values = np.exp(layer_outputs)\n",
    "    \n",
    "print(exp_values)\n",
    "#we then convert these values to a probability distribution\n",
    "#we do so by taking a given exp value and dividing it by the sum of all the other exp values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "\n",
    "print(norm_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "267dff27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's code a similar model for batches\n",
    "\n",
    "class activation_softmax:\n",
    "    def forward(self, inputs):\n",
    "        #we subtract the maximum value to deal with exploding values\n",
    "        exp_values = np.exp(inputs -np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "\n",
    "\n",
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                         [8.9, -1.81, 0.2],\n",
    "                         [1.41, 1.051, 0.026]])\n",
    "\n",
    "softmax = activation_softmax()\n",
    "softmax.forward(layer_outputs)\n",
    "\n",
    "np.argmax(softmax.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12fc1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n"
     ]
    }
   ],
   "source": [
    "X,y = spiral_data(samples=100, classes=3)\n",
    "dense1=dense_layer(2,3)\n",
    "activation1=activation_relu()\n",
    "dense2 = dense_layer(3,3)\n",
    "activation2 = activation_softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa11957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
